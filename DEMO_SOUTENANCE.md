# DÃ©mo Soutenance - 5 Minutes

**Date**: 12 janvier 2026  
**Statut**: âœ… PrÃªt pour prÃ©sentation

---

## ðŸŽ¯ Script de DÃ©monstration (5 minutes chrono)

### Minute 1: Architecture et Isolation

**Montrer:**
```bash
# Structure du projet
tree -L 2 services/
# â”œâ”€â”€ auth-service/
# â”œâ”€â”€ events-service/
# â”œâ”€â”€ messages-service/
# â””â”€â”€ shared/

# Ports dÃ©diÃ©s (isolation totale de Lify)
cat docker-compose.yml | grep -A 1 "ports:"
# PostgreSQL: 5433 (vs 5432 Lify)
# Redis: 6380 (vs 6379 Lify)
# MinIO: 9100/9101 (vs 9000/9001 Lify)
# Services: 4100, 4101, 4102
```

**Dire**: "3 microservices backend (Auth, Events, Messages) + infrastructure isolÃ©e"

---

### Minute 2: Docker Compose Local

```bash
# DÃ©marrer (si pas dÃ©jÃ  fait)
docker compose up -d

# Montrer tous les services running
docker compose ps
# NAME              STATUS
# lifytp_postgres   Up (healthy)
# lifytp_redis      Up (healthy)
# lifytp_minio      Up (healthy)
# lifytp_auth       Up
# lifytp_events     Up
# lifytp_messages   Up

# Tester un endpoint
curl http://localhost:4100/health
# {"status":"ok","service":"auth-service","port":4100}
```

**Dire**: "6 services dockerisÃ©s, testÃ©s localement avec health checks"

---

### Minute 3: Kubernetes - DÃ©ploiement

```bash
# VÃ©rifier le cluster
kubectl get nodes
# docker-desktop   Ready   v1.34.1

# Montrer le dÃ©ploiement
kubectl get all -n lifytp
# Pods: 8/8 Running (postgres, redis, 6x microservices)
# Services: 5 ClusterIP avec Load Balancing
# Deployments: 2 replicas par service

kubectl get pods -n lifytp
# auth-service-xxx (2/2 Running)
# events-service-xxx (2/2 Running)  
# messages-service-xxx (2/2 Running)
# postgres-0 (1/1 Running)
# redis-xxx (1/1 Running)
```

**Dire**: "DÃ©ploiement Kubernetes avec replicas pour haute disponibilitÃ©"

---

### Minute 4a: Auto-Healing (30 secondes)

```bash
# Terminal 1: Monitoring
kubectl get pods -n lifytp -l app=auth-service -w

# Terminal 2: Supprimer un pod
POD=$(kubectl get pod -n lifytp -l app=auth-service -o jsonpath='{.items[0].metadata.name}')
kubectl delete pod $POD -n lifytp
# pod "auth-service-xxx" deleted

# Observer:
# - Pod Terminating
# - Nouveau pod Created immÃ©diatement  
# - Running en ~9 secondes
# - Service toujours disponible (2Ã¨me replica continuait)
```

**Dire**: "Auto-healing dÃ©montrÃ©: pod supprimÃ© â†’ recrÃ©Ã© en 9s, zero downtime"

---

### Minute 4b: Rolling Update (30 secondes)

```bash
# DÃ©clencher update
kubectl patch deployment auth-service -n lifytp \
  -p '{"spec":{"template":{"metadata":{"annotations":{"version":"demo"}}}}}'

# Observer rollout
kubectl get pods -n lifytp -l app=auth-service -w
# Anciens pods terminÃ©s 1 par 1
# Nouveaux pods crÃ©Ã©s progressivement
# Toujours â‰¥1 pod Running
```

**Dire**: "Rolling update sans interruption: 2 replicas permettent zÃ©ro downtime"

---

### Minute 5: CI/CD et Conclusion

**Montrer workflows:**
```bash
# CI sur develop
cat .github/workflows/ci-develop.yml | grep -A 3 "jobs:"
# lint â†’ type-check â†’ build

# CD sur tags
cat .github/workflows/cd-main.yml | grep -A 5 "deploy-to-kubernetes:"
# build images â†’ push GHCR â†’ kubectl apply
```

**Conclure**:
- âœ… 3 microservices opÃ©rationnels
- âœ… Docker + Kubernetes fonctionnels
- âœ… Auto-healing et rolling updates dÃ©montrÃ©s
- âœ… CI/CD configurÃ©
- âœ… Isolation complÃ¨te de Lify
- âœ… Documentation exhaustive

---

## ðŸ“Š Checklist Avant PrÃ©sentation

### PrÃ©requis
- [ ] Cluster Kubernetes actif: `kubectl get nodes`
- [ ] Pods LifyTP running: `kubectl get pods -n lifytp`
- [ ] Docker Compose testÃ©: `docker compose ps`

### Commandes Ã  PrÃ©parer (copier-coller rapide)

**1. Docker Compose:**
```bash
docker compose ps
curl http://localhost:4100/health
```

**2. Kubernetes Status:**
```bash
kubectl get all -n lifytp
kubectl get pods -n lifytp
```

**3. Auto-Healing:**
```bash
# Terminal 1
kubectl get pods -n lifytp -l app=auth-service -w

# Terminal 2  
POD=$(kubectl get pod -n lifytp -l app=auth-service -o jsonpath='{.items[0].metadata.name}')
kubectl delete pod $POD -n lifytp
```

**4. Rolling Update:**
```bash
kubectl patch deployment auth-service -n lifytp \
  -p '{"spec":{"template":{"metadata":{"annotations":{"version":"'$(date +%s)'"}}}}}'
kubectl get pods -n lifytp -l app=auth-service -w
```

**5. Test Endpoints (si besoin):**
```bash
kubectl port-forward -n lifytp service/auth-service 4100:4100 &
curl http://localhost:4100/health
pkill -f port-forward
```

---

## ðŸŽ“ Points ClÃ©s Ã  Mentionner

### Architecture
- DÃ©coupage en 3 microservices based on domain-driven design
- Communication REST synchrone (extensible vers event-driven)
- Base de donnÃ©es partagÃ©e (simplification TP, Ã©volutif vers DB par service)

### Docker
- Multi-stage Dockerfiles optimisÃ©s (images <200MB)
- docker-compose.yml avec health checks et depends_on
- Isolation totale: ports, volumes, DB, networks dÃ©diÃ©s

### Kubernetes
- Namespace `lifytp` pour isolation
- Deployments avec 2 replicas (haute disponibilitÃ©)
- RollingUpdate strategy (maxUnavailable: 1)
- Health probes (liveness + readiness)
- Services ClusterIP avec Load Balancing

### CI/CD
- GitHub Actions: CI sur develop, CD sur tags
- Build automatique + push vers GHCR
- DÃ©ploiement Kubernetes automatisÃ© avec kubectl

### DÃ©mos
- **Auto-healing**: Kubernetes recrÃ©Ã© les pods supprimÃ©s (~9s)
- **Rolling update**: Mise Ã  jour sans downtime (progressive)
- **Zero downtime**: â‰¥1 pod toujours disponible grÃ¢ce aux replicas

---

## â±ï¸ Timing RecommandÃ©

| Ã‰tape | DurÃ©e | Contenu |
|-------|-------|---------|
| Introduction | 30s | Architecture microservices |
| Docker Compose | 1min | DÃ©mo locale, health checks |
| Kubernetes Status | 1min | Pods, services, replicas |
| Auto-Healing | 1min | Suppression pod + recrÃ©ation |
| Rolling Update | 1min | Update sans downtime |
| CI/CD + Conclusion | 1min30s | Workflows + rÃ©cap |

**Total**: 5-6 minutes

---

## ðŸš€ Plan B (si problÃ¨me technique)

**Si pods crashent:**
```bash
kubectl logs <POD_NAME> -n lifytp
kubectl describe pod <POD_NAME> -n lifytp
# Montrer les logs comme preuve de diagnostic mÃ©thodique
```

**Si cluster pas dispo:**
- Basculer sur Docker Compose (100% fonctionnel)
- Expliquer que K8s Ã©tait fonctionnel (showing captures d'Ã©cran)

**Si time overrun:**
- Sauter auto-healing OU rolling update (pas les 2)
- Garder 1 dÃ©mo minimum + CI/CD

---

## ðŸ“¸ Captures Ã  Avoir Sous la Main

1. **Auto-healing**: Pods Terminating â†’ Running
2. **Rolling update**: 2 versions de pods en parallÃ¨le
3. **kubectl get all -n lifytp**: Vue complÃ¨te du dÃ©ploiement
4. **docker compose ps**: Services locaux healthy

---

**TOUT EST PRÃŠT POUR LA SOUTENANCE!** ðŸŽ¯
